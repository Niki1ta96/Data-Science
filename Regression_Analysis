Regression Analysis: it is a conceptually simple method for investing functional relationship among variables.
-	This relationship is expressed in the form of equation or a model connecting the response variable to the one or more dependent or predictor variable.
-	 Y = f(X1, X2, ….., n) + €
Steps in Regression Analysis:
1.	Statement of Problem
2.	Selection of potentially relevant variable
3.	Data Collection 
4.	Model Specification 
5.	Choice of fitting methods
6.	Model fitting
7.	Model validation and criticism
8.	Using the chosen models for the solution of posed problems
Statement of Problem:
The most important step in regression coz,
-	Ill-defined problem or misformulated questions leads to wastage of efforts
-	May lead to wrong choices of the statistical method of analysis
-	Wrong choice of model
Selection of potentially relevant variable
-	Set a set of variables that are thought by experts 
Data Collection 
Data collection can be Qualitative or Quantitative
-	Qualitative (Category): Neighborhood Type (Good or Bad), House Style (Ranch, colonial, etc.)
-	Quantitative (Continuous numbers): House prices, Number of bed room, Tax, Age
-	Technique where the response variable is binary called as logistic regression
-	If all Predictors variables are qualitative then the technique is called as analysis of variance
-	If some of the predictor variables are quantitative while others are qualitative then the technique is called as analysis of covariance
Model Specification 
-	Linear:  Y = β0 + β1X1 + €
-	Non-Linear: Y = β0 + e(β1X1) + €   
-	A regression equation containing one predictor variable is called as simple regression equation
-	An equation containing more than one predictor variable is called as multiple regression equation
 

Model fitting
The estimated regression equation becomes,
  
The value Y hat is called as the fitted value.
Model validation and criticism
-	Models are built on certain assumptions about the data and the model. 
-	Accuracy of the analysis and the conclusion derived from the analysis depends on the validity of these assumptions 
 
Covariance and correlation coefficient:
We wish to measure the both the direction and strength of the variables
1.	Covariance: 
-	Cov (Y, X) = ∑ (Yi- Y^)(Xi – X^)/ n-1
-	Cov (Y, X) > 0, then positive relationship
-	Cov (Y, X) < 0, then negative relationship
-	Cov (Y, X) = 0, means there is no linear relationship, may or may not have any other form of relation
-	Cov is not scale dependent, it affected with the change in units 

 
2.	Correlation of coefficient:
-	Covariance between the standardized X and Y is called as correlation coefficient
-	It measures both the direction and the strength of relationship between the X and Y
-	Sign of correlation indicates the direction of the relationship
-	Cor (Y,X) > 0, implies X and Y are positively related
-	Cor(Y, X) < 0, implies X and Y are negatively related 
-	Cor(Y, X) = 0 means they are not linearly related
-	Magnitude of Cor(Y,X) measures strength
-	Correlation is useful quantity to measures both strength and directions of the linear relationship between the Y and X
-	-1 <= Cor(Y, X) <= 1
-	
 

Simple Linear Regression Analysis:
-	Simple linear regression is an approach for predicting a quantitative response using a single feature (or "predictor" or "input variable"). It takes the following form:
	Y = β0 + β1X1 + €
Where, 
Y is the response
x is the feature
β0 is the intercept
β1 is the coefficient of x
Together β0 and β1 is the model coefficient

-	Estimating ("Learning") Model Coefficients
coefficients are estimated using the least squares criterion, which means we are find the line (mathematically) which minimizes the sum of squared residuals (or "sum of squared errors"
 

Where,
The black dots are the observed values of x and y
The blue line is our least squares line
The red lines are the residuals, which are the distances between the observed values and the least squares line.

-	How do the model coefficients relate to the least squares line?
β0 is the intercept (the value of y when x=0)
β1 is the slope (the change in y divided by change in x)
-	Graphical depiction of those calculations
 



Confidence in our Model:
- Is linear regression a high bias/low variance model, or a low bias/high variance model
- High bias/low variance:  Under repeated sampling, the line will stay roughly in the same place (low variance), but the average of those models won't do a great job capturing the true relationship (high bias). 
- Note that low variance is a useful characteristic when you don't have a lot of training data!
-  A closely related concept is confidence intervals.
-  Statsmodels calculates 95% confidence intervals for our model coefficients, which are interpreted as follows, If the population from which this sample was drawn was sampled 100 times, approximately 95 of those confidence intervals would contain the "true" coefficient.
Hypothesis Testing and p-values:
-	Closely related to confidence intervals is hypothesis testing. 
-	We start with a null hypothesis and an alternative hypothesis (that is opposite the null).
-	 Then, we check whether the data supports rejecting the null hypothesis or failing to reject the null hypothesis.
-	Note that "failing to reject" the null is not the same as "accepting" the null hypothesis.
 The alternative hypothesis may indeed be true, except that you just don't have enough data to show that.)
-	As it relates to model coefficients, here is the conventional hypothesis test:
null hypothesis:  There is no relationship between TV ads and Sales (and thus β1 equals to zero)
alternative hypothesis:  There is a relationship between TV ads and Sales (and thus β1 is not equal to zero)
-	How do we test this hypothesis? Intuitively, we reject the null (and thus believe the alternative) if the 95% confidence interval does not include zero. Conversely, the p-value represents the probability that the coefficient is actually zero
-	If the 95% confidence interval includes zero, the p-value for that coefficient will be greater than 0.05. If the 95% confidence interval does not include zero, the p-value will be less than 0.05. Thus, a p-value less than 0.05 is one way to decide whether there is likely a relationship between the feature and the response. (Again, using 0.05 as the cutoff is just a convention.)
-	For example, the p-value for TV is far less than 0.05, and so we believe that there is a relationship between TV ads and Sales.
-	Note that we generally ignore the p-value for the intercept




How Well Does the Model Fit the data?¶
The most common way to evaluate the overall fit of a linear model is by the R-squared value. R-squared is the proportion of variance explained, meaning the proportion of variance in the observed data that is explained by the model, or the reduction in error over the null model. (The null model just predicts the mean of the observed response, and thus it has an intercept and no slope.)
R-squared is between 0 and 1, and higher is better because it means that more variance is explained by the model. Here's an example of what R-squared "looks like"
 
The blue line explains some of the variance in the data (R-squared=0.54), the green line explains more of the variance (R-squared=0.64), and the red line fits the training data even further (R-squared=0.66). (Does the red line look like it's overfitting?)  YES
Difference between correlation coefficient and Regression analysis:
-	Cor(Y, X) is same as Cor(X, Y)
-	In Cor, X and Y are equally importance
-	In regression, Y is primarily importance
How to validate the assumption of linear regression,
1.	Parameter estimator, used least square method to minimize the error
2.	Test of hypothesis, if X = 0 then Y = β0 + €
3.	Confidence interval, 
Multiple Linear Regression:
Y = β0 + β1X1 + β2X2 + …. + βnXn + €
Where, β0, β1, … , constants or regression coefficient
€ - random error






  




 
